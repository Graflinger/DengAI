{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../dengue_features_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-991ae3261bae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mfeatures_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../dengue_features_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../dengue_labels_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mfeatures_output_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../dengue_features_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'../dengue_features_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "\n",
    "features_train = pd.read_csv(\"dengue_features_train.csv\")\n",
    "labels_train = pd.read_csv(\"dengue_labels_train.csv\")\n",
    "features_output_data = pd.read_csv(\"dengue_features_test.csv\")\n",
    "features_output_format = pd.read_csv(\"submission_format.csv\")\n",
    "\n",
    "kc_data = pd.DataFrame(features_train, columns=['city',\n",
    "  'year',\n",
    " 'weekofyear',\n",
    " # 'ndvi_ne',\n",
    " # 'ndvi_nw',\n",
    " # 'ndvi_se',\n",
    " # 'ndvi_sw',\n",
    " 'precipitation_amt_mm',\n",
    " 'reanalysis_air_temp_k',\n",
    " 'reanalysis_avg_temp_k',\n",
    " 'reanalysis_dew_point_temp_k',\n",
    " 'reanalysis_max_air_temp_k',\n",
    " 'reanalysis_min_air_temp_k',\n",
    " 'reanalysis_precip_amt_kg_per_m2',\n",
    " 'reanalysis_relative_humidity_percent',\n",
    " 'reanalysis_sat_precip_amt_mm',\n",
    " 'reanalysis_specific_humidity_g_per_kg',\n",
    " 'reanalysis_tdtr_k',\n",
    " 'station_avg_temp_c',\n",
    " 'station_diur_temp_rng_c',\n",
    " 'station_max_temp_c',\n",
    " 'station_min_temp_c',\n",
    " 'station_precip_mm'])\n",
    "kc_data['total_cases'] = pd.DataFrame(labels_train,columns=['total_cases'])\n",
    "#kc_data['week_start_date'] = pd.to_numeric(features_train.week_start_date.str.slice(8, 10))\n",
    "\n",
    "kc_data_out = pd.DataFrame(features_output_data, columns=['city',\n",
    " 'year',\n",
    " 'weekofyear',\n",
    " # 'ndvi_ne',\n",
    " # 'ndvi_nw',\n",
    " # 'ndvi_se',\n",
    " # 'ndvi_sw',\n",
    " 'precipitation_amt_mm',\n",
    " 'reanalysis_air_temp_k',\n",
    " 'reanalysis_avg_temp_k',\n",
    " 'reanalysis_dew_point_temp_k',\n",
    " 'reanalysis_max_air_temp_k',\n",
    " 'reanalysis_min_air_temp_k',\n",
    " 'reanalysis_precip_amt_kg_per_m2',\n",
    " 'reanalysis_relative_humidity_percent',\n",
    " 'reanalysis_sat_precip_amt_mm',\n",
    " 'reanalysis_specific_humidity_g_per_kg',\n",
    " 'reanalysis_tdtr_k',\n",
    " 'station_avg_temp_c',\n",
    " 'station_diur_temp_rng_c',\n",
    " 'station_max_temp_c',\n",
    " 'station_min_temp_c',\n",
    " 'station_precip_mm'])\n",
    "\n",
    "# Separate data into cities\n",
    "\n",
    "iq_data = kc_data.loc[kc_data.city == 'iq']\n",
    "iq_data_out = kc_data_out.loc[kc_data_out.city == 'iq']\n",
    "\n",
    "iq_data.drop('city', axis=1)\n",
    "iq_data_out.drop('city', axis=1)\n",
    "iq_data.fillna(iq_data.interpolate(), inplace=True)\n",
    "iq_data_out.fillna(iq_data_out.interpolate(), inplace=True)\n",
    "\n",
    "sj_data = kc_data.loc[kc_data.city == 'sj']\n",
    "sj_data_out = kc_data_out.loc[kc_data_out.city == 'sj']\n",
    "sj_data.drop('city', axis=1)\n",
    "sj_data_out.drop('city', axis=1)\n",
    "sj_data.fillna(sj_data.interpolate(), inplace=True)\n",
    "sj_data_out.fillna(sj_data_out.interpolate(), inplace=True)\n",
    "\n",
    "kc_data.drop('city',axis=1)\n",
    "kc_data.drop('city',axis=1)\n",
    "\n",
    "label_col = 'total_cases'\n",
    "print(kc_data.describe())\n",
    "\n",
    "def train_validate_test_split(df, train_part, validate_part, test_part, seed=None):\n",
    " np.random.seed(seed)\n",
    " total_size = train_part + validate_part + test_part\n",
    " train_percent = train_part / float(total_size)\n",
    " validate_percent = validate_part / float(total_size)\n",
    " test_percent = test_part / total_size\n",
    " perm = np.random.permutation(df.index)\n",
    " m = len(df)\n",
    " train_end = int(train_percent * m)\n",
    " validate_end = int(validate_percent * m) + train_end\n",
    " train = perm[:train_end]\n",
    " validate = perm[train_end:validate_end]\n",
    " test = perm[validate_end:]\n",
    " return train, validate, test\n",
    "\n",
    "#train_size, valid_size, test_size = (70, 25, 5)\n",
    "kc_train, kc_valid, kc_test = train_validate_test_split(kc_data,\n",
    " train_part=100,\n",
    " validate_part=0,\n",
    " test_part=0,\n",
    " seed=2020)\n",
    "\n",
    "kc_y_train = kc_data.loc[kc_train, [label_col]]\n",
    "kc_x_train = kc_data.loc[kc_train, :].drop(label_col, axis=1)\n",
    "kc_y_valid = kc_data.loc[kc_valid, [label_col]]\n",
    "kc_x_valid = kc_data.loc[kc_valid, :].drop(label_col, axis=1)\n",
    "# kc_out_test = kc_data_out.drop('city', axis=1)\n",
    "\n",
    "print('Size of training set: ', len(kc_x_train))\n",
    "print('Size of validation set: ', len(kc_x_valid))\n",
    "print('Size of test set: ', len(kc_test), '(not converted)')\n",
    "\n",
    "def norm_stats(df1, df2):\n",
    "    dfs = df1.append(df2)\n",
    "    minimum = np.min(dfs)\n",
    "    maximum = np.max(dfs)\n",
    "    mu = np.mean(dfs)\n",
    "    sigma = np.std(dfs)\n",
    "    return (minimum, maximum, mu, sigma)\n",
    "\n",
    "def norm_stats_single(df1):\n",
    "    dfs = df1\n",
    "    minimum = np.min(dfs)\n",
    "    maximum = np.max(dfs)\n",
    "    mu = np.mean(dfs)\n",
    "    sigma = np.std(dfs)\n",
    "return (minimum, maximum, mu, sigma)\n",
    "\n",
    "def z_score(col, stats):\n",
    "    m, M, mu, s = stats\n",
    "    df = pd.DataFrame()\n",
    "    for c in col.columns:\n",
    "        df[c] = (col[c]-mu[c])/s[c]\n",
    "    return df\n",
    "\n",
    "stats = norm_stats(kc_x_train, kc_x_valid)\n",
    "stats_out = norm_stats_single(kc_data_out)\n",
    "arr_x_train = np.array(z_score(kc_x_train, stats))\n",
    "# arr_x_train['city'] = pd.DataFrame(features_train, columns=['city']).\n",
    "arr_y_train = np.array(kc_y_train)\n",
    "arr_x_valid = np.array(z_score(kc_x_valid, stats))\n",
    "#arr_y_train['city'] = pd.DataFrame(features_train,columns=['city'])\n",
    "arr_y_valid = np.array(kc_y_valid)\n",
    "\n",
    "arr_output = np.array(z_score(kc_data_out,stats_out))\n",
    "\n",
    "print('Training shape:', arr_x_train.shape)\n",
    "print('Training samples: ', arr_x_train.shape[0])\n",
    "print('Validation samples: ', arr_x_valid.shape[0])\n",
    "\n",
    "def basic_model_1(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
    "    t_model.add(Dense(50, activation=\"relu\"))\n",
    "    t_model.add(Dense(y_size))\n",
    "    print(t_model.summary())\n",
    "    t_model.compile(loss='mean_squared_error',\n",
    "    optimizer=Adam(),\n",
    "    metrics=[metrics.mae])\n",
    "    return(t_model)\n",
    "\n",
    "def basic_model_2(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(100, activation=\"tanh\", input_shape=(x_size,)))\n",
    "    t_model.add(Dropout(0.1))\n",
    "    t_model.add(Dense(50, activation=\"relu\"))\n",
    "    t_model.add(Dense(20, activation=\"relu\"))\n",
    "    t_model.add(Dense(y_size))\n",
    "    print(t_model.summary())\n",
    "    t_model.compile(loss='mean_squared_error',\n",
    "    optimizer=Adam(),\n",
    "    metrics=[metrics.mae])\n",
    "    return(t_model)\n",
    "\n",
    "def basic_model_3(x_size, y_size):\n",
    "    t_model = Sequential()\n",
    "    t_model.add(Dense(x_size, activation=\"tanh\", kernel_initializer='normal', input_shape=(x_size,)))\n",
    "    t_model.add(Dropout(0.2))\n",
    "    t_model.add(Dense(int(x_size*2), activation=\"relu\", kernel_initializer='normal',\n",
    "    kernel_regularizer=regularizers.l1(1e-5), bias_regularizer=regularizers.l1(1e-5)))\n",
    "    t_model.add(Dropout(0.3))\n",
    "    t_model.add(Dense(int(x_size*.75), activation=\"relu\", kernel_initializer='normal',\n",
    "    kernel_regularizer=regularizers.l1_l2(1e-3), bias_regularizer=regularizers.l1_l2(1e-3)))\n",
    "    t_model.add(Dropout(0.2))\n",
    "    t_model.add(Dense(int(y_size*7), activation=\"relu\", kernel_initializer='normal'))\n",
    "    t_model.add(Dropout(0.0))\n",
    "    t_model.add(Dense(y_size))\n",
    "    t_model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='nadam',\n",
    "    metrics=[metrics.mae])\n",
    "    return(t_model)\n",
    "\n",
    "model = basic_model_3(arr_x_train.shape[1], arr_y_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "\n",
    "print('Epochs: ', epochs)\n",
    "print('Batch size: ', batch_size)\n",
    "\n",
    "keras_callbacks = [\n",
    " # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=True, verbose=2)\n",
    " # ModelCheckpoint('/tmp/keras_checkpoints/model.{epoch:02d}.hdf5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    " TensorBoard(log_dir='/tmp/keras_logs/model_84', write_graph=True, write_images=True),\n",
    " EarlyStopping(monitor='mean_absolute_error', patience=50, verbose=0)\n",
    "]\n",
    "\n",
    "history = model.fit(arr_x_train, arr_y_train,\n",
    " batch_size=batch_size,\n",
    " epochs=epochs,\n",
    " shuffle=True,\n",
    " verbose=0, # Change it to 2, if wished to observe execution\n",
    " #validation_data=(arr_x_valid, arr_y_valid),\n",
    " callbacks=keras_callbacks)\n",
    "\n",
    "\n",
    "train_score = model.evaluate(arr_x_train, arr_y_train, verbose=0)\n",
    "#valid_score = model.evaluate(arr_x_valid, arr_y_valid, verbose=0)\n",
    "\n",
    "print('Train MAE: ', round(train_score[1], 4), ', Train Loss: ', round(train_score[0], 4))\n",
    "# print('Val MAE: ', round(valid_score[1], 4), ', Val Loss: ', round(valid_score[0], 4))\n",
    "\n",
    "output = model.predict(arr_output)\n",
    "features_output_format.drop('total_cases', axis=1)\n",
    "features_output_format['total_cases'] = np.rint(output).astype(int)\n",
    "\n",
    "features_output_format.to_csv('dengai_predictions_neural_networks_new4.csv', index=False)\n",
    "\n",
    "def plot_hist(h, xsize=6, ysize=10):\n",
    "    # Prepare plotting\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    plt.rcParams[\"figure.figsize\"] = [xsize, ysize]\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, sharex=True)\n",
    "\n",
    "    # summarize history for MAE\n",
    "    plt.subplot(211)\n",
    "    plt.plot(h['mean_absolute_error'])\n",
    "    plt.plot(h['val_mean_absolute_error'])\n",
    "    plt.title('Training vs Validation MAE')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(212)\n",
    "    plt.plot(h['loss'])\n",
    "    plt.plot(h['val_loss'])\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot it all in IPython (non-interactive)\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "plot_hist(history.history, xsize=8, ysize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
